{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf00d16-27b5-48d3-aae0-dc21793fa707",
   "metadata": {},
   "source": [
    "# Slurm jobs on NeSI GPUs\n",
    "\n",
    "In this section, we will revisit our first model and try to execute it non-interactively on NESI's GPUs, using a Slurm job.\n",
    "\n",
    "\n",
    "## Introduction to HPC\n",
    "\n",
    "An HPC (high performance computing) platform is a set of nodes put together with fast interconnection, in order to execute massively parallel computations or many different computations in parallel.\n",
    "\n",
    "<img src=\"../images/mahuika_maui_real.png\" width=900 />\n",
    "\n",
    "Each computation is a **job**, which will be run non-interactively on a set of nodes that possess the right amount of CPU, memory and GPU requested by the user, for a given period of time.\n",
    "All jobs are handled by a **scheduler**, that queue and assign them to nodes depending on available resources and requests.\n",
    "NeSI uses **Slurm** (Simple Linux Utility for Resource Management) for this.\n",
    "\n",
    "In this workshop, we will submit a job via Slurm to train a model using a node that has a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd58c6-9163-4dc6-98c4-5559bcfd6765",
   "metadata": {},
   "source": [
    "## Create a Python script\n",
    "\n",
    "We will see in a moment how to send a job request to Slurm using a *job submission script*.\n",
    "Before this, we need to convert our code into a self-contained script that can be run from this job submission script, like a command line executable.\n",
    "The following cell will write a python script file containing the code to train the example model from the [Image classification](02_classification.ipynb) notebook.\n",
    "\n",
    "The last line `model.save(\"outputs/trained_model_flowers\")` ensures that the trained model is saved at the end of the script, here in a folder called `outputs/trained_model_flowers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb686e3c-86b2-493f-8254-d1bbfadca3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/train_model.py\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define the hyper-parameters of the model\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "\n",
    "# load training dataset and split it in train, validation and test sets\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file(\"flower_photos\", origin=dataset_url, untar=True)\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "num_classes = 5\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "val_batches = tf.data.experimental.cardinality(val_ds)\n",
    "test_ds = val_ds.take(val_batches // 2)\n",
    "val_ds = val_ds.skip(val_batches // 2)\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# model definition, using data augmentation during training\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\", input_shape=(img_height, img_width, 3)),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        data_augmentation,\n",
    "        layers.Rescaling(1.0 / 255),\n",
    "        layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(num_classes),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary(line_length=80)\n",
    "\n",
    "# compile and train the model\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n",
    "\n",
    "# evaluate the model and plot learning curves\n",
    "test_loss, test_acc = model.evaluate(test_ds, verbose=2)\n",
    "print(f\"test accuracy: {test_acc}\")\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "ax1.plot(epochs_range, history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "ax1.plot(epochs_range, history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "ax1.axhline(test_acc, color=\"k\", label=\"Test Accuracy\")\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax1.set_title(\"Training and Validation Accuracy\")\n",
    "\n",
    "ax2.plot(epochs_range, history.history[\"loss\"], label=\"Training Loss\")\n",
    "ax2.plot(epochs_range, history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "ax2.axhline(test_acc, color=\"k\", label=\"Test Loss\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "ax2.set_title(\"Training and Validation Loss\")\n",
    "\n",
    "fig.savefig(\"outputs/learning_curves.png\")\n",
    "\n",
    "# save the model on disk\n",
    "model.save(\"outputs/trained_model_flowers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3584959e-73b7-447c-93cf-561ceead470b",
   "metadata": {},
   "source": [
    "If you check the `scripts` folder, you should now see a file called `train_model.py`.\n",
    "\n",
    "*Note: JupyterLab also provides a text editor that can be used create and edit text files. Click on this [link](scripts/train_model.py) to open it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4a6ecc-1330-4933-ad50-e5a999063ea2",
   "metadata": {},
   "source": [
    "## Create a Slurm job script\n",
    "\n",
    "To tell Slurm how to run our code, we need to write a **job submission script** whose role is to:\n",
    "\n",
    "1. detail the resources requirements (CPUs, RAM, GPU, time limit, etc.),\n",
    "1. setup the software environment,\n",
    "1. run our script.\n",
    "\n",
    "The job submission script is a regular *bash script* with:\n",
    "\n",
    "1. a header with comments to tell Slurm about the resources we want,\n",
    "1. some command using *environment modules* to load the right software stack for us,\n",
    "1. a bash command to run our script\n",
    "\n",
    "<img src=\"../images/anatomyofslurm_bashscript.png\" width=900 />\n",
    "\n",
    "Execute the following cell to create the job submission script `train_model.sl`:\n",
    "\n",
    "1. This job will request 2 CPUs, 8 GB of RAM and a A100-1g.5gb GPU (1/7th of a A100) for 10 minutes using the training account `nesi99991`.\n",
    "1. The software stack for this workshop has been prepared as a conda environment, hence we need to load the conda module and activate the conda environment.\n",
    "1. The final line of the script run our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe7582a-e7c1-4e58-894c-b7db3034c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/train_model.sl\n",
    "#!/usr/bin/env bash\n",
    "#SBATCH --account=nesi99991\n",
    "#SBATCH --time=00-00:10:00\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --mem=8GB\n",
    "#SBATCH --gpus-per-node=A100-1g.5gb:1\n",
    "#SBATCH --output=logs/slurm-%j.out\n",
    "\n",
    "# load required environment modules\n",
    "module purge\n",
    "module load Miniconda3 cuDNN/8.1.1.33-CUDA-11.2.0\n",
    "\n",
    "# activate the conda environment\n",
    "source $(conda info --base)/etc/profile.d/conda.sh\n",
    "export PYTHONNOUSERSITE=1\n",
    "conda deactivate\n",
    "conda activate /nesi/project/nesi99991/ml102_20220616/jupyter_kernel_env\n",
    "\n",
    "# execute the script\n",
    "python scripts/train_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b442462-a1d6-4b96-9db8-842cd9fe0548",
   "metadata": {},
   "source": [
    "## Submit a Slurm job and check results\n",
    "\n",
    "Now that we have a training script and job submission script, we can submit our job to Slurm and wait for the results üéâ.\n",
    "\n",
    "To interact with Slurm, there is a set of command line tools:\n",
    "\n",
    "- `sbatch JOBSCRIPT` to submit the job script to Slurm, and returns a **JOB ID** number,\n",
    "- `squeue --me` to check the position of your jobs in the Slurm queue,\n",
    "- `scancel JOBID` to cancel your job,\n",
    "- etc.\n",
    "\n",
    "<img src=\"../images/batch_system_flow.png\" width=900 />\n",
    "\n",
    "You can enter them in a terminal running on NeSI, in JupyterLab or an SSH session.\n",
    "\n",
    "Here we will use an alternative, using the syntax `!command` in a notebook running on NeSI.\n",
    "\n",
    "First, let's submit our job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f6d9a-324e-4893-9555-507ada233813",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch scripts/train_model.sl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b684d-396e-4e74-87e2-3e9a0416a44b",
   "metadata": {},
   "source": [
    "The `sbatch` command returns a number, the job ID, that is unique to this computation.\n",
    "\n",
    "Next, let's see if your job has already started or is waiting for resources, using the `squeue` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6888318-47ee-4b13-bf7a-6d3514cbfe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!squeue --me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34168e07-3eac-455e-8dea-f06c3bf324c4",
   "metadata": {},
   "source": [
    "Because the current Jupyter session is also running as a Slurm job, you should see at least one job called `spawner-jupy`.\n",
    "\n",
    "If it's the only one, then your job has finished.\n",
    "‚ö†Ô∏è It does not mean that your job has been successful.\n",
    "To check the final status of your job, you can:\n",
    "\n",
    "- look at the log file called `slurm-JOBID.out`, which captures everything that your Python script would have printed on the command line,\n",
    "- use the command `sacct -j JOBID` to print the status of your job (FAILED, COMPLETED, etc.)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_ml102",
   "language": "python",
   "name": "tensorflow_ml102"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
